{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\l l\\Documents\\SentimentWatch\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS\n",
    "import praw\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Initialize PRAW\n",
    "reddit = praw.Reddit(client_id='srDUoJOjy8WI8ul2CCUiUQ',\n",
    "                     client_secret='p0ZGHduBE9bdeB0mSt-0o4kSfw4BsQ',\n",
    "                     user_agent='web:my_reddit_analyzer:v1.0 (by /u/BellaGiulia)')\n",
    "\n",
    "# Initialize VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@app.route('/reddit_sentiment', methods=['GET'])\n",
    "def reddit_sentiment():\n",
    "    offset = int(request.args.get('offset', 0))\n",
    "    hot_posts = reddit.subreddit('TrueOffMyChest').hot(params={\"after\": offset}, limit=10)\n",
    "    sentiments = []\n",
    "\n",
    "    for post in hot_posts:\n",
    "        analysis = sia.polarity_scores(post.title)\n",
    "        content_analysis = sia.polarity_scores(post.selftext)\n",
    "        sentiments.append({\n",
    "            'title': post.title,\n",
    "            'positive': analysis['pos'],\n",
    "            'negative': analysis['neg'],\n",
    "            'neutral': analysis['neu'],\n",
    "            'compound': analysis['compound'],\n",
    "            'score': str(analysis),\n",
    "            'content': post.selftext,\n",
    "            'content_sentiment': str(content_analysis),\n",
    "            'content_sentiment2': content_analysis['compound'],\n",
    "        })\n",
    "\n",
    "    return jsonify(sentiments)\n",
    "\n",
    "@app.route('/search_subreddits', methods=['GET'])\n",
    "def search_subreddits():\n",
    "    topic = request.args.get('topic', '')\n",
    "    comment_limit = request.args.get('comment_limit', 10, type=int)  # Default to 10 comments\n",
    "    search_results = reddit.subreddit('all').search(topic, limit=10)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in search_results:\n",
    "        post.comments.replace_more(limit=0)  # Load all comment threads\n",
    "        comments = []\n",
    "        for comment in post.comments[:comment_limit]:  # Fetch only the first few comments\n",
    "            doc = nlp(comment.body)\n",
    "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            sentiment = sia.polarity_scores(comment.body)\n",
    "            comments.append({\n",
    "                'body': comment.body,\n",
    "                'sentiment': sentiment,\n",
    "                'entities': entities\n",
    "            })\n",
    "        posts_data.append({\n",
    "            'title': post.title + \"\\nContent:\" + \"\\n\" + post.selftext,\n",
    "            'comments': comments\n",
    "        })\n",
    "\n",
    "    return jsonify(posts_data)\n",
    "\n",
    "\n",
    "@app.route('/fetch_comments', methods=['GET'])\n",
    "def fetch_comments():\n",
    "    subreddit_name = request.args.get('subreddit', '')\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    comments_entities = []\n",
    "\n",
    "    for post in subreddit.hot(limit=10):\n",
    "        post.comments.replace_more(limit=0)\n",
    "        for comment in post.comments.list():\n",
    "            doc = nlp(comment.body)\n",
    "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            sentiment = sia.polarity_scores(comment.body)\n",
    "            comments_entities.append({\n",
    "                'body': comment.body,\n",
    "                'sentiment': sentiment,\n",
    "                'entities': entities\n",
    "            })\n",
    "\n",
    "    return jsonify(comments_entities)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
